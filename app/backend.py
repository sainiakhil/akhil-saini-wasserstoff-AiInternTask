import os
from langchain.document_loaders import PyPDFLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter


import pytesseract
from PIL import Image
from langchain.schema import Document
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.prompts import ChatPromptTemplate
import sqlite3
from langchain.vectorstores import FAISS


def init_sqlite_db():
    conn = sqlite3.connect('db')
    cursor = conn.cursor()

    cursor.execute('''
        CREATE TABLE IF NOT EXISTS chunks (
            doc_id TEXT NOT NULL,
            chunk_index INTEGER NOT NULL,
            text TEXT NOT NULL,
            page_number INTEGER,
            paragraph_approx INTEGER
        )
    ''')
    conn.commit()
    return conn


model_name = "sentence-transformers/all-MiniLM-L6-v2"
embedding_model = HuggingFaceEmbeddings(model_name=model_name)
llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash-001", google_api_key="AIzaSyC1sSrDhxP395Ece502bpVP8eDOtPvTkro", temperature=0.2)




def load_and_process_pdfs(pdf_folder_path):
    conn = init_sqlite_db()
    documents = []
    for file in os.listdir(pdf_folder_path):
        if file.endswith('.pdf'):
            pdf_path = os.path.join(pdf_folder_path, file)
            loader = PyPDFLoader(pdf_path)
            documents.extend(loader.load())
        elif file.endswith('.txt'):
            txt_path = os.path.join(pdf_folder_path, file)
            loader = TextLoader(txt_path)
            documents.extend(loader.load())
        elif file.endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp')):
          img_path = os.path.join(pdf_folder_path, file)
          text = pytesseract.image_to_string(Image.open(img_path))
          documents.append(Document(page_content=text, metadata={'source': img_path}))

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    
    all_chunks_data = []
    chunk_texts_for_embedding = []
    chunk_idx_counter = 0
    doc_counter = 1 # Initialize document counter

    for doc in documents:
        page_num = doc.metadata.get("page", 0)  # Get page number from metadata
        page_text = doc.page_content
        
        if not page_text.strip():
            continue

        # Generate doc_id
        doc_id = f"DOC{doc_counter:03d}"
        doc_counter += 1 # Increment document counter for the next document

        page_chunks = text_splitter.split_text(page_text)
        para_approx_counter = 0
        for chunk_text in page_chunks:
            para_approx_counter += 1
            all_chunks_data.append(
                (doc_id, chunk_idx_counter, chunk_text, page_num, para_approx_counter) # Changed to tuple
            )
            chunk_texts_for_embedding.append(Document(page_content=chunk_text, metadata={'page': page_num, 'paragraph_approx': para_approx_counter, 'doc_id': doc_id, 'chunk_index': chunk_idx_counter}))

            chunk_idx_counter += 1
       
    # Move insertion outside the loop to insert all at once
    cursor = conn.cursor()
    cursor.executemany("INSERT INTO chunks (doc_id, chunk_index, text, page_number, paragraph_approx) VALUES (?, ?, ?, ?, ?)", all_chunks_data)
    conn.commit()
    conn.close()
    print("Successfully added all chunks to chunks table")
    return chunk_texts_for_embedding # Return the processed chunks

def initialize_vectorstore(all_splits):

    return FAISS.from_documents(documents=all_splits, embedding=embedding_model)


# prompt: code for printing the reterived chunks based user input with their meta data and final answer generated by LLM


def get_chunks_and_answer(query, vectorstore, top_k=3):
    """
    Returns retrieved chunks with metadata and LLM's final answer.
    """
    docs = vectorstore.similarity_search(query, k=top_k)
    
    conn = sqlite3.connect('db')
    cursor = conn.cursor()

    retrieved_chunks = []
    for doc in docs:
        # Fetch metadata from SQLite
        cursor.execute("SELECT * FROM chunks WHERE doc_id=? AND chunk_index=?", (doc.metadata['doc_id'], doc.metadata['chunk_index']))
        chunk_data = cursor.fetchone()
        
        if chunk_data:
            chunk_info = {
                "text": doc.page_content,
                "doc_id": chunk_data[0],
                "page_number": chunk_data[3],
                "paragraph_approx": chunk_data[4],
            }
            retrieved_chunks.append(chunk_info)

    conn.close()

    # Generate final answer
    prompt_template = """
    You are an expert at synthesizing information from multiple documents. You will be provided with context snippets and their document IDs. Answer the user's question clearly and concisely, citing the DocID in parentheses `(DocID: X)` immediately following the statement or information it supports. 
    Use all relevant DocIDs. If you don't know the answer, just say that you don't know. Don't try to make up an answer.

    {context}

    Question: {question}

"""
    prompt = ChatPromptTemplate.from_template(prompt_template)
    messages = prompt.format_messages(
        context="\n\n".join([chunk["text"] for chunk in retrieved_chunks]),
        question=query
    )
    final_answer = llm(messages).content

    return retrieved_chunks, final_answer


def limit_text(text, word_limit=50):
    words = text.split()
    return " ".join(words[:word_limit]) + ("..." if len(words) > word_limit else "")

